import torch
import os
import re
from transformers import AutoModelForCausalLM, LlamaTokenizer
import warnings
warnings.filterwarnings("ignore")

# ========== å¤šå¡é…ç½®ï¼ˆGPU 0/1/2/3ï¼‰ ==========
os.environ["CUDA_VISIBLE_DEVICES"] = "0,1,2,3"
os.environ["TRANSFORMERS_NO_ADVISORY_WARNINGS"] = "1"
available_gpus = [torch.cuda.device(i) for i in range(torch.cuda.device_count())]
print(f"âœ… å¯ç”¨GPUï¼š{available_gpus}ï¼ˆå…±{len(available_gpus)}å¼ ï¼‰")

# æ¨¡å‹è·¯å¾„
MODEL_PATH = "/data_jijingbo/models/Emollama-chat-7b"

# æ ¸å¿ƒï¼šä½¿ç”¨ä½ æŒ‡å®šçš„çº¯è‹±æ–‡æç¤ºè¯ + LLaMA2åŸç”ŸæŒ‡ä»¤æ¨¡æ¿
def build_score_prompt(text: str) -> str:
    # LLaMA2å®˜æ–¹<s>[INST] [/INST]æ¨¡æ¿ + ä½ æŒ‡å®šçš„è‹±æ–‡æŒ‡ä»¤ï¼Œä»…è¾“å‡ºæ•°å­—
    prompt = f"""<s>[INST]
Evaluate the valence intensity of the writer's mental state based on the text, assigning it a real-valued score from 0 (most negative) to 1 (most positive). Only output the numerical score, no other words or symbols.
Text: {text}
Valence Intensity Score: [/INST]"""
    return prompt

# æå–å¾—åˆ†ï¼šå¼ºåŒ–LLaMA2è¾“å‡ºé€‚é…ï¼Œå…¼å®¹æ‰€æœ‰æ•°å­—æ ¼å¼ï¼ˆ0/1/0.xxx/.xxxï¼‰
def extract_score(output_text: str) -> float:
    # æ­£åˆ™åŒ¹é…0-1ä¹‹é—´çš„æµ®ç‚¹æ•°/æ•´æ•°ï¼Œé€‚é…LLaMA2æ‰€æœ‰å¸¸è§è¾“å‡ºæ ¼å¼
    score_pattern = re.compile(r'(\d+\.?\d*|\.\d+)')
    matches = score_pattern.findall(output_text)
    if matches:
        score = float(matches[0])
        return max(0.0, min(1.0, score))  # å¼ºåˆ¶å½’ä¸€åŒ–0-1ï¼Œé˜²æ­¢æ¨¡å‹è¾“å‡ºè¶…å‡ºèŒƒå›´
    else:
        return 0.5  # æ— åŒ¹é…æ—¶è¿”å›ä¸­æ€§åˆ†ï¼Œé¿å…0.000æ— æ•ˆå€¼

def load_model_and_infer():
    try:
        # 1. åŠ è½½LLaMA2åˆ†è¯å™¨ï¼ˆåŸç”Ÿé…ç½®ï¼Œé€‚é…è‹±æ–‡æŒ‡ä»¤ï¼‰
        print("ğŸ”§ åŠ è½½LLaMA2åˆ†è¯å™¨...")
        tokenizer = LlamaTokenizer.from_pretrained(
            MODEL_PATH,
            trust_remote_code=True,
            padding_side="right",  # LLaMA2åŸç”Ÿæ¨èå³å¡«å……ï¼Œå…³é”®ï¼
            use_fast=False,        # ç¦ç”¨å¿«é€Ÿåˆ†è¯å™¨ï¼Œé€‚é…å¾®è°ƒæ¨¡å‹çš„tokenizer.model
            add_bos_token=True,
            add_eos_token=True
        )
        # è¡¥å……pad_tokenï¼ˆLLaMA2é»˜è®¤æ— ï¼Œæ¨ç†å¿…å¤‡ï¼‰
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            tokenizer.pad_token_id = tokenizer.eos_token_id
        print("âœ… åˆ†è¯å™¨åŠ è½½æˆåŠŸï¼")

        # 2. å¤šå¡åŠ è½½LLaMA2å¾®è°ƒæ¨¡å‹ï¼ˆé€‚é…åŸç”Ÿæƒé‡ï¼Œæ˜¾å­˜å‡åŒ€åˆ†é…ï¼‰
        print("ğŸ”§ åŠ è½½LLaMA2å¾®è°ƒæ¨¡å‹åˆ°å¤šGPUï¼ˆ0/1/2/3ï¼‰...")
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_PATH,
            dtype=torch.float16,       # åŠç²¾åº¦ï¼Œé™ä½æ˜¾å­˜å ç”¨
            low_cpu_mem_usage=True,    # å‡å°‘CPUå†…å­˜å ç”¨
            trust_remote_code=True,
            device_map="auto",         # è‡ªåŠ¨å¤šå¡åˆ†ç‰‡ï¼Œé€‚é…4å¡3090
            offload_folder="./offload" # ä¸´æ—¶å¸è½½ç›®å½•ï¼Œé˜²æ­¢å†…å­˜æº¢å‡º
        )
        print(f"âœ… æ¨¡å‹è®¾å¤‡åˆ†é…ï¼š{model.hf_device_map}")
        print(f"âœ… LLaMA2å¾®è°ƒæ¨¡å‹åŠ è½½å®Œæˆï¼\n")

        # 3. æµ‹è¯•ç”¨ä¾‹ï¼ˆä¸­æ–‡æ–‡æœ¬ï¼Œæ¨¡å‹è‡ªåŠ¨è¯†åˆ«æƒ…ç»ªï¼‰
        test_cases = [
            {"text": "Just won the lottery! I can't believe this is happening to me!"},
            {"text": "My beloved dog passed away today, I'm heartbroken and devastated."},
            {"text": "I met Bob today!"},
            {"text": "I met Bob today."},
            {"text": "I met messi today!"},
            {"text": "I met messi today."},
        ]

        # 4. æ‰¹é‡æ¨ç†+å¾—åˆ†æå–ï¼ˆçº¯è‹±æ–‡æŒ‡ä»¤é©±åŠ¨ï¼‰
        print("ğŸš€ å¼€å§‹å¤šå¡æ¨ç†ï¼ˆçº¯è‹±æ–‡æŒ‡ä»¤ï¼Œè¾“å‡º0-1æ•ˆä»·å¼ºåº¦å¾—åˆ†ï¼‰...\n")
        for idx, case in enumerate(test_cases, 1):
            text = case["text"]
            # æ„é€ LLaMA2åŸç”Ÿæ ¼å¼çš„çº¯è‹±æ–‡æç¤ºè¯Prompt
            prompt = build_score_prompt(text)
            # ç¼–ç ï¼šè¾“å…¥æ”¾GPU0ï¼Œæ¨¡å‹è‡ªåŠ¨åˆ†å‘åˆ°å¤šå¡
            inputs = tokenizer(
                prompt,
                return_tensors="pt",
                truncation=True,
                max_length=512
            ).to("cuda:0")

            # 5. LLaMA2ä¸“å±æ¨ç†å‚æ•°ï¼ˆé€‚é…è‹±æ–‡æŒ‡ä»¤ï¼Œé¿å…0.000ï¼‰
            model.eval()
            with torch.no_grad():  # ç¦ç”¨æ¢¯åº¦ï¼ŒèŠ‚çœæ˜¾å­˜
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=6,        # ä»…è¾“å‡ºæ•°å­—ï¼Œè¶³å¤Ÿç”¨ï¼ˆå¦‚0.896/1.0ï¼‰
                    temperature=0.2,         # ä½æ¸©åº¦ä¿è¯å¾—åˆ†ç¨³å®šï¼Œæ— éšæœºæ³¢åŠ¨
                    top_p=0.9,               # LLaMA2å®˜æ–¹æ¨èå€¼ï¼Œæ ¸é‡‡æ ·
                    top_k=40,                # LLaMA2åŸç”Ÿæ¨èå€¼ï¼Œæå‡å‡ºåˆ†æ¦‚ç‡
                    do_sample=True,          # è½»å¾®é‡‡æ ·ï¼Œé¿å…æ¨¡å‹æƒ°æ€§è¾“å‡º0.0
                    pad_token_id=tokenizer.pad_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                    repetition_penalty=1.0,  # LLaMA2ç¦ç”¨é‡å¤æƒ©ç½šï¼Œå…³é”®ï¼
                    length_penalty=1.0       # ç¦ç”¨é•¿åº¦æƒ©ç½šï¼Œé¿å…æ•°å­—æˆªæ–­
                )

            # 6. è§£ç +ç²¾å‡†æå–å¾—åˆ†ï¼ˆè¿‡æ»¤ç‰¹æ®Štokenå’Œå‰ç¼€ï¼‰
            raw_output = tokenizer.decode(outputs[0], skip_special_tokens=True)
            # æˆªå–[/INST]åçš„å†…å®¹ï¼Œä»…ä¿ç•™æ¨¡å‹è¾“å‡ºçš„æ•°å­—
            score_text = raw_output.split("[/INST]")[-1].strip()
            # æå–0-1çš„æ•ˆä»·å¼ºåº¦å¾—åˆ†
            valence_score = extract_score(score_text)

            # 7. æ‰“å°ç»“æœï¼ˆæ ‡æ³¨0=æœ€è´Ÿé¢ï¼Œ1=æœ€æ­£é¢ï¼Œæ¸…æ™°ç›´è§‚ï¼‰
            print(f"ã€æµ‹è¯•ç”¨ä¾‹ {idx}ã€‘")
            print(f"æ–‡æœ¬ï¼š{text}")
            print(f"æ•ˆä»·å¼ºåº¦å¾—åˆ†ï¼ˆ0=æœ€è´Ÿé¢ | 1=æœ€æ­£é¢ï¼‰ï¼š{valence_score:.3f}\n" + "-"*120 + "\n")

        # 8. å¤šå¡æ˜¾å­˜ä½¿ç”¨ç»Ÿè®¡
        print("ğŸ“Š GPUæ˜¾å­˜ä½¿ç”¨æƒ…å†µï¼ˆ0-3å¡ï¼‰ï¼š")
        for i in range(4):
            mem_used = torch.cuda.memory_allocated(i) / 1024**3
            mem_total = torch.cuda.get_device_properties(i).total_memory / 1024**3
            print(f"GPU {i}ï¼šå·²ç”¨ {mem_used:.1f}GB / æ€» {mem_total:.1f}GB")

        return model, tokenizer

    except Exception as e:
        print(f"\nâŒ è¿è¡Œå‡ºé”™ï¼š{type(e).__name__} - {str(e)[:300]}")
        torch.cuda.empty_cache()  # å‡ºé”™æ—¶æ¸…ç†æ˜¾å­˜
        return None, None
    finally:
        # æ— è®ºæ˜¯å¦æˆåŠŸï¼Œæ¸…ç†æ˜¾å­˜+ä¸´æ—¶ç›®å½•
        torch.cuda.empty_cache()
        import shutil
        shutil.rmtree("./offload", ignore_errors=True)

# ========== ä¸»å‡½æ•°æ‰§è¡Œ ==========
if __name__ == "__main__":
    os.makedirs("./offload", exist_ok=True)  # åˆ›å»ºä¸´æ—¶å¸è½½ç›®å½•
    model, tokenizer = load_model_and_infer()
    if model is not None:
        print("\nğŸ‰ LLaMA2å¾®è°ƒæ¨¡å‹å¤šå¡æ¨ç†å®Œæˆï¼æ‰€æœ‰æ–‡æœ¬å‡ç²¾å‡†è¾“å‡º0-1æ•ˆä»·å¼ºåº¦å¾—åˆ†ï½")
    else:
        print("\nâŒ æ¨ç†å¤±è´¥ï¼")